---
title: "BE2 – Hydrological modeling - Session 2"
author: "Hammad Khalid, Umar Alpha, Zohaib Saleem, Md Golam Sarwar"
date: "12 December 2022"
output:
  word_document: default
  html_document:
    df_print: paged
subtitle: "Part 2 – Calibration and sensitivity analysis of a rainfall-runoff model"
bibliography: references.bib
---

```{r,include=FALSE}
#If include=FALSE, knitr will not display the code in the code chunk
#If echo=TRUE, knitr will display the result of executing the code in the code chunk
knitr::opts_chunk$set(echo = TRUE) # sets the option echo=T for the entire document
```

**This is the framework to write the documents expected for BE2 Engineering Hydrology (sessions 1 and 2).**

**You will drop your reports after these two sessions in Chamilo (ENSE3 Engineering Hydrology (4EUS3HYI) , in 'Travaux', [by 13/01/2023 at the very latest.]{.underline}**

**The reports will be graded and will account for the continuous assessment.**

**The following working documents are expected:**

-   ***the completed RMarkdown file for session 1 and 2***

-   ***the raw Word file generated from the RMarkdown file for session 1 and 2***

-   ***the final report in PDF format including:***

    -   cover page, headers and footers, logos, table of contents, table of figures and tables, bibliography

    -   a summary of your results organized according to the following plan :

        -   [*introduction*]{.underline} : list the main objectives of the BE, introduce the hydrological model used and specify in which model category it belongs, add a diagram of the model and mention its time step, the input data necessary for its use, the number of parameters and their definition, and its output

        -   [*river basin and data*]{.underline} : give a description of the river basin (name, area, median elevation, graph of the hypsometric curve), a synthetic description of the available time series, a graph of time-series, perform a simple water balance and runoff analysis, and add a description of the hydrological regime with graph

        -   [*calibration criterion*]{.underline}: give a description of NSE and KGE2, explain the preference for KGE2, give performances for NSE(Q), KGE2(Q) and NSE(1/Q), explain the interest of a change of variable, indicate and justify your final choice

        -   [*warmup period*]{.underline}: explain the experiment and specify which period represents the best compromise

        -   [*calibration-validation*]{.underline}: explain the experiment. Specify the added value of this method and which parameters you have chosen

        -   [*simulation phase*]{.underline}: give and comment the model performance in the simulation phase, in terms of KGE

        -   [*sensitivity analysis*]{.underline}: explain the relevance of this part, explain the method and give the results, precise the most sensitive parameters

        -   [*use in forecasting*]{.underline}: answer the questions asked in this paragraph

------------------------------------------------------------------------

[**Objective of this session:**]{.underline}

-   perform rainfall-runoff simulations with the R package airGRteaching

-   Understand the different steps of hydrological modelling\
    (calibration, validation, simulation and sensitivity analysis)

***R commands are available but their arguments have to be filled in.\
(replace "..." with the right argument)***

Check that the airGRteaching library is installed.

------------------------------------------------------------------------

## Data preparation

1.  Read data

    Set your working directory and check it

```{r eval=FALSE, include=FALSE}
    setwd("F:/Ense3/1. 1st Semester/Engineering Hydrology/3. BE Project/BE2")
    getwd("F:/Ense3/1. 1st Semester/Engineering Hydrology/3. BE Project/BE2")
```

    Load the airGRteaching library

    ```{r}
    library("airGRteaching")
    ```

    Read the data included in the airGRteaching package for the basin L0123001

    ```{r}
    data(L0123001)
    ```

    [Use session 1 of the BE to produce information and write a presentation of the basin and the sample data.]{.ul}

2.  Select the useful observed data for rainfall-runoff modelling\
    (dates for delimitating the modelling period, input precipitation and potential evapotranspiration data, output discharge data in mm) to fit the type and order of inputs expected by the argument obsDF of function "PrepGR"

    ```{r}
    BasinObs2 <- BasinObs[,c("DatesR", "P", "E", "Qmm","T")]
    ```

3.  Prepare data for a GR4J model without the snow module

    ```{r}
    PREP <- PrepGR(ObsDF = BasinObs2, HydroModel = "GR4J", CemaNeige = FALSE)
    ```

4.  Plot data [in order to insert it in your report]{.underline}.

    ```{r}
    plot(PREP,main = "L0123001")
    ```

## Calibration criterion

In this section we will analyze the sensitivity of the parameter calibration to the chosen calibration criterion.

Two calibration criteria are proposed in airGRteaching: the Nash-Sutcliffe Efficiency (NSE) and the modified Kling-Gupta Efficiency (KGE2):

$$
NSE = 1-\frac{\sum_{i=1}^{n}(X_i-Y_i)^{2}}{\sum_{i=1}^{n}(X_i-\mu_X)^{2}}
$$ With $i$ the time step, $n$ the total number of time steps available for calibration, $X_i$ the observed discharge and $\mu_X$ its mean, and $Y_i$ the simulated discharge

$$
KGE2=1-\sqrt{(1-r)^2+(\beta-1)^2+(\gamma-1)^2}
$$

With $\beta$ the model bias, $r$ the coefficient of correlation and $\gamma$ the ratio of the simulated and observed coefficients of variation

$$
\beta=\frac{\mu_Y}{\mu_X}
$$

$$
\gamma=\frac{CV_Y}{CV_X}=\frac{\frac{\sigma_Y}{\mu_Y}}{\frac{\sigma_X}{\mu_X}}
$$

$$
r=\frac{\sum_{i=1}^{n}(X_i-\mu_X).(Y_i-\mu_Y)}{\sqrt{\sum_{i=1}^{n}(X_i-\mu_X)^{2}}.\sqrt{\sum_{i=1}^{n}(Y_i-\mu_Y)^{2}}}
$$

1.  [Explain why the KGE2 criterion seems to be more relevant than NSE (focus on its formulation):]{.underline}

NSE can be defined as one minus the sum of the absolute squared differences between the predicted and observed values normalized by the variance of the observed values series would have been a better predictor than the model. The largest disadvantage of the Nash-Sutcliffe efficiency is the fact that the differences between the observed and predicted values are calculated as squared values. As a result larger values in a time series are strongly overestimated whereas lower values are neglected . For the quantification of runoff predictions this leads to an overestimation of the model performance during peak flows and an underestimation during low flow conditions. Whereas, KGE2 is an updated version of KGE09 (which is diagonal decomposition of NSE) [@jackson2019] and this metric separates the different aspects of the flow regime that is correlation ($r$), the bias($\beta$) , and a measure of relative variability($\gamma$) so if the value of the criteria is small then it can be identified that which parameter needs adjustment.

2.  Select the entire available period for calibration, except the first year that will be used as warmup period (default method when the WupPer parameter in airGRteaching is null):

    ```{r}
    PERIOD = c("1985-01-01", "2012-12-31")
    ```

3.  Calibrate the model over the chosen reference period, on the basis of the NSE criterion.\

    ```{r}
    CAL_NSE <- CalGR(PrepGR = PREP, CalCrit = "NSE", CalPer = PERIOD, WupPer = NULL)
    ```

    What is the optimum NSE value ?\
    What are the associated GR4J parameter values (X1, X2, X3 and X4) ?

    ```{r}
    CAL_NSE$OutputsCalib$CritFinal
    CAL_NSE$OutputsCalib$ParamFinalR
    ```

    Plot the overall performance results and export the graph in order to insert it in your report:

    ```{r}
    plot(CAL_NSE,main = "L0123001 - GR4J - NSE")
    ```

    From the above performance curve visually it is difficult to observe the difference in observed and simulated time series. However from the other 3 graphs we can deduce that, The rolling mean for both the series superimpose each other which means that both follow the same trend as per seasonality. Also the cumulative non-exceedance probability for both the trends are almost same except the low flow portion where they differ this aspect is very important as we can estimate the probabilities with sheer confidence for high flows but not for low flows. From the scatter plot between the observed and simulated flow we can say that the values are overestimated for the stream flows, as since most of the values are above 1:1 line.

4.  Calibrate the model over the chosen reference period, on the basis of the KGE2 criterion.\
    What is the optimum KGE2 value ?\
    What are the associated GR4J parameter values ?

    ```{r}
    CAL_KGE2 <- CalGR(PrepGR = PREP, CalCrit = "KGE2", CalPer = PERIOD, WupPer = NULL)
    ```

    ```{r}
    CAL_KGE2$OutputsCalib$CritFinal
    CAL_KGE2$OutputsCalib$ParamFinalR
    ```

    Plot the overall performance results and export the graph to insert it in your report:

    ```{r}
    plot(CAL_KGE2, main="L0123001 - GR4J - KGE")
    ```

    From the above performance curve visually it is difficult to observe the difference in observed and simulated time series. However from the other 3 graphs we can deduce that, The rolling mean for both the series do not superimpose each other but the trend they follow are same which explains that both the series do respect the trend of seasonality. Furthermore, cummulative non-exceedance probability for both the trends superimpose each other which is very important as we can estimate the probabilities with sheer confidence for low flows and high flows. From the scatter plot between the observed and simulated flow we can say that the values are still overestimated for the stream flows, since most of the values are above 1:1 line, however if we compare it to the case in NSE it has improved a bit.

5.  Calibrate the model over the chosen reference period, on the basis of the NSE criterion calculated on inverse discharge (1/Q).\
    What is the optimum NSE(1/Q) value?\
    What are the associated GR4J parameter values?

    ```{r}
    CAL_NSE_INV <- CalGR(PrepGR = PREP, CalCrit = "NSE", CalPer = PERIOD, transfo = "inv")
    ```

    ```{r}
    CAL_NSE_INV$OutputsCalib$CritFinal
    CAL_NSE_INV$OutputsCalib$ParamFinalR
    ```

    Plot the overall performance results and export the graph to insert it in your report:

    ```{r}
    plot(CAL_NSE_INV, main ="L0123001 - GR4J - NSE(1/Q)")
    ```

    From the above performance curve visually it is difficult to observe the difference in observed and simulated time series. However from the other 3 graphs we can deduce that, The rolling mean for both the series do not superimpose each other but the trend they follow are same which explains that both the series do respect the trend of seasonality. Furthermore, cumulative non-exceedance probability for both the trends superimpose each other which is very important as we can estimate the probabilities with sheer confidence for low flows and high flows. From the scatter plot between the observed and simulated flow we can say that the values approximately equally divided between 1:1 line and it is much improved than NSE and KGE2 case.

    [Try to explain why this change of variable improves the calibration result.]{.underline}.   As the NSE belongs to the family of mean model square error criteria, so it puts more emphasis on high flows and puts more weight on high flows, therefore transformation of variable to inverse results in more weight on low flows.

Complete the table below. Compare the calibration performances and the different optimized sets of parameter values at questions 3 to 5. [Indicate and justify your preference.]{.underline}

|               |   NSE(Q)    |  KGE2(Q)   |  NSE(1/Q)  |
|---------------|:-----------:|:----------:|:----------:|
| **Criterion** |  0.7956564  | 0.8548429  | 0.7134102  |
| **X1**        | 215.9226249 | 131.630664 | 172.431490 |
| **X2**        |  0.5994375  |  0.000000  |  0.000000  |
| **X3**        | 99.1834071  | 68.033484  | 44.701184  |
| **X4**        |  2.1808447  |  2.295796  |  2.168919  |

Although KGE2 gives the highest percentage of closeness between the observed and simulated flows but the NSE(1/Q) is more adequate for low and high flows which could also be observed from the scatter plot graph between observed and simulated flows for NSE(1/Q) as the data is scattered equally between the 1:1 line. Hence the preference will be to use NSE(1/Q) criteria.

[\
]{.underline}Are flood discharges well represented in the simulations obtained (look at the graphs)? Try to give possible explanations.\

Yes the flood discharges are well represented in the simulations obtained from NSE, KGE2 and NSE(1/Q), this observation has been derived based on the performance graphs plotted for the three criterion, we can see that the Non-exceedance probability for high flows in all the 3 criteria are predicted accurately which affirms this observation.

Do you think that calculating the NSE or the KGE2 criterion only during simulation periods containing discharges that are above a given threshold value could improve the model calibration?\
Yes I believe that this scenario can improve the model calibration as NSE and KGE2 are more inclined towards high flows.

What could be the impact on the low water values? But with this scenario, the parameters obtained will not be suitable for low flows, neither the model will perform good low flows.

***The KGE2 criterion will be used thereafter:***

```{r}
CRIT = "KGE2"
```

## Warmup period

One of the important issues in hydrological modelling is to specify the initial conditions of the catchment, since it has a major impact on the catchment response, and thus on the model response. The model warmup is an adjustment process for the model to reach an 'optimal' state, where internal stores move from the guessed initial condition to an 'optimal' state. The objective of the following series of questions is to evaluate the impact of the length of the warmup period on the calibration results.

By default, the production store of the GR4J model is initialized at a level of 30% and the routing store at a level of 50%. The values simulated during the warmup period are not included in the calculation of the criterion. By default, this period is fixed as the first year before the beginning of the period of calibration (or simulation).

1.  Select the period from 01/01/1994 to 31/12/1994 for the calibration

    ```{r}
    PERIOD2 = c("1994-01-01", "1994-12-31")
    ```

2.  Define 4 warmup periods, respectively of 1 month, 6 months, 1 year and 2 years:

    ```{r}
    WP1 = c("1993-12-01", "1993-12-31")
    WP2 = c("1993-06-01", "1993-12-31")
    WP3 = c("1993-01-01", "1993-12-31")
    WP4 = c("1992-01-01", "1993-12-31")
    ```

3.  Calibrate the model with the 4 different warmup periods

    ```{r}
    CAL_WP1 <- CalGR(PrepGR = PREP, CalCrit = CRIT, WupPer = WP1, CalPer = PERIOD2)
    CAL_WP2 <- CalGR(PrepGR = PREP, CalCrit = CRIT, WupPer = WP2, CalPer = PERIOD2)
    CAL_WP3 <- CalGR(PrepGR = PREP, CalCrit = CRIT, WupPer = WP3, CalPer = PERIOD2)
    CAL_WP4 <- CalGR(PrepGR = PREP, CalCrit = CRIT, WupPer = WP4, CalPer = PERIOD2)
    ```

4.  Compare the results in terms of KGE and parameter values.\
    [Specify which period represents the best compromise.]{.underline}

    ```{r}
    #final value of the criterion
    CAL_WP1$OutputsCalib$CritFinal
    CAL_WP2$OutputsCalib$CritFinal
    CAL_WP3$OutputsCalib$CritFinal
    CAL_WP4$OutputsCalib$CritFinal
    #final parameter set
    CAL_WP1$OutputsCalib$ParamFinalR
    CAL_WP2$OutputsCalib$ParamFinalR
    CAL_WP3$OutputsCalib$ParamFinalR
    CAL_WP4$OutputsCalib$ParamFinalR
    ```

    Complete the table

    |               |    WP1     |     WP2     |     WP3     |     WP4     |
    |---------------|:----------:|:-----------:|:-----------:|:-----------:|
    | **Criterion** | 0.9029425  |  0.9180561  |  0.9181854  |  0.9181857  |
    | **X1**        | 112.892529 | 179.0798368 | 182.3404560 | 182.3404560 |
    | **X2**        |  0.254166  |  0.3241509  |  0.3300811  |  0.3300811  |
    | **X3**        | 118.955316 | 99.1784463  | 98.1027648  | 98.1027648  |
    | **X4**        |  2.187605  |  2.187605   |  2.2572024  |  2.2572024  |

    Is the default duration for warmup period (1 year) a good compromise between calibration performance and calculation time ?\
    We can say that it is not such a reasonable compromise as, the shortest warm up period (1 month) has an efficiency of 90.30 percent and takes 24 iterations, while the 1 year warm up period gives an efficiency of 91.81 percent only 2 percent more for 4 additional iterations which does not seems a reasonable compromise.

    Has the warm up period the same influence on a short and a long calibration period ?

    No the warm up period would not have the same influence on a short and long calibration period, as with the warm up period we intend to remove the initialization bias, hence if the period is short and the model takes long to get into equilibrium than it can affect the calibration output as the calibration output might get sensitive to the initialization biasness.

## Calibration-validation

In order to verify and improve the robustness of the calibration, the available data set is generally separated into two sub-periods used successively for calibration and then for validation.

1.  Select two sub-periods of 10 years at the beginning and the end of the available data set\
    (excluding the first year used as the warm up period).

    ```{r}
    P1 = c("1985-01-01", "1994-12-31")
    P2 = c("2003-01-01", "2012-12-31")
    ```

    Are the two sub-periods equivalent from a hydrological point of view ? (look at the module, the low waters, the flood discharges)

From the module, Low and High water flows we can say that the sub-periods are not that equivalent as there is a significant change between the 3 statistical descriptors.

    ```{r}
    BasinObsP1=BasinObs2[BasinObs2$DatesR>=P1[1] & BasinObs2$DatesR<=P1[2],]
    BasinObsP2=BasinObs2[BasinObs2$DatesR>=P2[1] & BasinObs2$DatesR<=P2[2],]
    summary(BasinObsP1)
    summary(BasinObsP2)
    ```

2.  Calibrate the model on the first sub-period and validate it on the second one

    ```{r}
    CAL1_P1 <- CalGR(PrepGR = PREP, CalCrit = CRIT, CalPer = P1)
    SIM1_P2 <- SimGR(PrepGR = PREP, CalGR = CAL1_P1, EffCrit = CRIT, SimPer = P2)
    ```

3.  Calibrate the model on the second sub-period and validate it on the first one

    ```{r}
    CAL2_P2 <- CalGR(PrepGR = PREP, CalCrit = CRIT, CalPer = P2)
    SIM2_P1 <- SimGR(PrepGR = PREP, CalGR = CAL2_P2, EffCrit = CRIT, SimPer = P1)
    ```

4.  Compare the results obtained at questions 2 and 3 in terms of KGE values (both in the calibration and the validation sub-periods), as well as in terms of calibrated parameter values. Comment.

    ```{r}
    #Calibration - final value of the criterion
    CAL1_P1$OutputsCalib$CritFinal
    CAL2_P2$OutputsCalib$CritFinal
    #Calibration - final parameter set
    CAL1_P1$OutputsCalib$ParamFinalR
    CAL2_P2$OutputsCalib$ParamFinalR
    #Validation - value of the criterion
    SIM1_P2$EffCrit$CritValue
    SIM2_P1$EffCrit$CritValue
    ```

    If we observe that for the case in Question 2 the KGE2 gives a value of **86.35%** for calibration and for validation it gives **74.90%**, which in the case of Question 3, KGE2 value for both calibration and validation increases i.e **88.05** and **77.77%** respectively.

Complete the table.

|                           |     P1     |     P2      |
|---------------------------|:----------:|:-----------:|
| **Calibration Criterion** | 0.8635565  |  0.8805101  |
| **X1**                    | 117.919242 | 164.4637009 |
| **X2**                    |  0.487496  | -0.5339851  |
| **X3**                    | 76.707539  | 75.1581575  |
| **X4**                    |  2.198198  |  2.2886748  |
| **Simulation Criterion**  | 0.7490331  |  0.777487   |

[What are the best calibrated parameter values? Explain and justify]{.underline}.

The difference between KGE values for Calibration and Validation for the case in Question 3 is smaller hence it means that the parameters obtained are robust hence the parameters related to it are the best calibration parameters.

## Simulation phase

The objective is to simulate the discharge corresponding to the observed precipitation and evapotranspiration during the period that has not been considered for the calibration-validation phase. We will use as representative parameter values the ones obtained by calibrating the model in the more recent sub-period:

```{r}
CAL = CAL2_P2
PCAL = P2
X1= CAL$OutputsCalib$ParamFinalR[1]
X2= CAL$OutputsCalib$ParamFinalR[2]
X3= CAL$OutputsCalib$ParamFinalR[3]
X4= CAL$OutputsCalib$ParamFinalR[4]
PARM = c(X1,X2,X3,X4)
```

1.  Specify the simulation period:

    ```{r}
    PSIM= c("1995-01-01", "2002-12-31")
    ```

2.  Simulate the discharge on this period:

    ```{r}
    SIM <- SimGR(PrepGR = PREP, Param = CAL, EffCrit = CRIT, SimPer = PSIM)
    ```

3.  Plot the result.

    ```{r}
    plot(SIM, main="Simulation")
    ```

[What is the model performance in the simulation phase, in terms of KGE? Comment.]{.underline}

Visually it is difficult to infer much from the observed and simulated time series.. However from the other 3 graphs we can deduce that,in-spite that the rolling mean for both the series do not superimpose each other but the trend they follow are same which explains that both the series do respect the trend of seasonality. Furthermore, non-exceedance probability for both the trends superimpose each other which is very important as we can estimate the probabilities with sheer confidence for low flows and high flows. From the scatter plot between the observed and simulated flow we can say that the values are approximately equally divided between 1:1 line. Also if we compare the KGE value of this period **(79.73%)** to the KGE2 value(**77.755 %**) for simulation of the period ("2003-01-01", "2012-12-31") using the same parameters we can also observe that the parameters simulate the other periods with the same confidence. Hence the model simulations are a robust one.

## Sensitivity analysis

The objective is to determine the model sensitivity to the values of its parameters (X1, X2, X3 and X4).

Run the next function to add it to your list of available functions and to use it in the next code chunk.

```{r}
#Function SummaryGR
#extract main data from airGRteaching object
SummaryGR<-function(ObjGR){
        SGR = NULL
        if(is(ObjGR)=="CalGR"){
                TypeRun = "CAL"
                TypeModel = ObjGR$TypeModel
                CritName = ObjGR$OutputsCalib$CritName
                CritValue = round(ObjGR$OutputsCalib$CritFinal,4)
                RunFirst = as.character(ObjGR$PeriodModel$Run[1])
                RunLast = as.character(ObjGR$PeriodModel$Run[2])
                WarmUpFisrt = as.character(ObjGR$PeriodModel$WarmUp[1])
                WarmUpLast = as.character(ObjGR$PeriodModel$WarmUp[2])
                X1 = round(ObjGR$OutputsCalib$ParamFinalR[1],2)
                X2 = round(ObjGR$OutputsCalib$ParamFinalR[2],4)
                X3 = round(ObjGR$OutputsCalib$ParamFinalR[3],2)
                X4 = round(ObjGR$OutputsCalib$ParamFinalR[4],3)
                SGR = data.frame(TypeRun,TypeModel,CritName,CritValue,
                                 RunFirst,RunLast,WarmUpFisrt,WarmUpLast,
                                 X1,X2,X3,X4)
        }else if(is(ObjGR)=="SimGR"){
                TypeRun = "SIM"
                TypeModel = ObjGR$TypeModel
                CritName = ObjGR$EffCrit$CritName
                CritValue = round(ObjGR$EffCrit$CritValue,4)
                RunFirst = as.character(ObjGR$PeriodModel$Run[1])
                RunLast = as.character(ObjGR$PeriodModel$Run[2])
                WarmUpFisrt = as.character(ObjGR$PeriodModel$WarmUp[1])
                WarmUpLast = as.character(ObjGR$PeriodModel$WarmUp[2])
                X1 = NA
                X2 = NA
                X3 = NA
                X4 = NA
                SGR = data.frame(TypeRun,TypeModel,CritName,CritValue,
                                RunFirst,RunLast,WarmUpFisrt,WarmUpLast,
                                X1,X2,X3,X4)
                
        }
        return(SGR)
}

```

Hereafter , we will keep the calibrated parameter values over the more recent sub-period (i.e. CAL2_P2) and will evaluate the sensitivity of the calibration (PCAL) on such parameter values.

Display the calibrated parameter values:

```{r}
print(PARM)
```

### X1 parameter

1.  We will first work on the X1 parameter values:

    ```{r}
    name="X1"
    i=as.integer(substr(name, start = 2, stop =2))
    ```

2.  Try to define an interval of parameter values to be tested as -/+50% around the nominal value of the X1 parameter, in the form of a sequence (min, max, step):

    ```{r}
    Xmin=PARM[i]*0.5
    Xmax=PARM[i]*1.5
    dX = (Xmax-Xmin)/10
    rX=seq(Xmin,Xmax,dX)
    ```

3.  Run the simulation on the values to be tested**:**

    ```{r}
    SGR_X=data.frame()
    for(X in rX){
            P=PARM
            P[i]=X
            S <- SimGR(PrepGR = PREP,Param = P, EffCrit = CRIT, SimPer = PCAL)
            SX = SummaryGR(S)
            SX$X1=P[1]
            SX$X2=P[2]
            SX$X3=P[3]
            SX$X4=P[4]
            SX[,name]=X
            SGR_X=rbind(SGR_X,SX)
    }
    ```

4.  Plot the relationship between each parameter value and its corresponding KGE criterion value:

    ```{r}
    plot(SGR_X[,9],SGR_X$CritValue,
         main="Sensitivity study",
         ylab=CRIT,xlab=name)
    points(PARM[i],CAL$OutputsCalib$CritFinal,col="black",pch=18,cex=2)
    ```

Perform similar sensitivity analyses on the other parameter values (X2, X3 and X4). If you want, you can use a loop on parameters.

### X2 parameter

```{r}
#repeat the analysis as for parameter X2
 name="X2"
    i=as.integer(substr(name, start = 2, stop =2))
    
Xmin=PARM[i]*0.5
    Xmax=PARM[i]*1.5
    dX = (Xmax-Xmin)/10
    rX=seq(Xmin,Xmax,dX)
    
SGR_X=data.frame()
    for(X in rX){
            P=PARM
            P[i]=X
            S <- SimGR(PrepGR =PREP ,Param = P, EffCrit = CRIT, SimPer = PCAL)
            SX = SummaryGR(S)
            SX$X1=P[1]
            SX$X2=P[2]
            SX$X3=P[3]
            SX$X4=P[4]
            SX[,name]=X
            SGR_X=rbind(SGR_X,SX)
    }


 plot(SGR_X[,10],SGR_X$CritValue,
         main="Sensitivity study",
         ylab=CRIT,xlab=name)
    points(PARM[i],CAL$OutputsCalib$CritFinal,col="black",pch=18,cex=2)
```

### X3 parameter

```{r}
#repeat the analysis as for parameter X3
name="X3"
    i=as.integer(substr(name, start = 2, stop =2))
    
Xmin=PARM[i]*0.5
    Xmax=PARM[i]*1.5
    dX = (Xmax-Xmin)/10
    rX=seq(Xmin,Xmax,dX)
    
SGR_X=data.frame()
    for(X in rX){
            P=PARM
            P[i]=X
            S <- SimGR(PrepGR =PREP ,Param = P, EffCrit = CRIT, SimPer = PCAL)
            SX = SummaryGR(S)
            SX$X1=P[1]
            SX$X2=P[2]
            SX$X3=P[3]
            SX$X4=P[4]
            SX[,name]=X
            SGR_X=rbind(SGR_X,SX)
    }


 plot(SGR_X[,11],SGR_X$CritValue,
         main="Sensitivity study",
         ylab=CRIT,xlab=name)
    points(PARM[i],CAL$OutputsCalib$CritFinal,col="black",pch=18,cex=2)
```

### X4 parameter

```{r}
#repeat the analysis as for parameter X4
name="X4"
    i=as.integer(substr(name, start = 2, stop =2))
    
Xmin=PARM[i]*0.5
    Xmax=PARM[i]*1.5
    dX = (Xmax-Xmin)/10
    rX=seq(Xmin,Xmax,dX)
    
SGR_X=data.frame()
    for(X in rX){
            P=PARM
            P[i]=X
            S <- SimGR(PrepGR =PREP ,Param = P, EffCrit = CRIT, SimPer = PCAL)
            SX = SummaryGR(S)
            SX$X1=P[1]
            SX$X2=P[2]
            SX$X3=P[3]
            SX$X4=P[4]
            SX[,name]=X
            SGR_X=rbind(SGR_X,SX)
    }


 plot(SGR_X[,12],SGR_X$CritValue,
         main="Sensitivity study",
         ylab=CRIT,xlab=name)
    points(PARM[i],CAL$OutputsCalib$CritFinal,col="black",pch=18,cex=2)
```

[What are the most sensitive parameters?]{.underline} The most sensitive parameter is X3 as it varies the most by changing the other parameters (the middle one).

## Use in forecasting

Can the model be used for forecasting discharges in real time ?

No the model cannot be used forecasting discharge, but there is GPR model which is widely used in France for flood forecasting, but the tool is not available in AirGR now.

If yes, specify the type of input data that are necessary for this purpose in warmup and in forecasting mode.

Do you think it is possible to use observed discharge to correct forecast, and how would you used them ?
